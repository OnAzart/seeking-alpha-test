# Task:
Load data. API -> datalake -> data warehouse.

# Overview
That's a common data task, which have a bunch of different solutions depends on different intricacies of requirements.

Some of the possible solutions comming to my mind:
- Airflow orchestrating EMR job for large-scale data, or containers for smaller jobs;
- Step Functions orchestrating Lambdas and loading to Redshift;
- Glue with Glue Workflows;

There are a lot to consider before choosing:
- Size of data, possible transformations, cost, etc;

Let's assume that we have small job, without extra transforming, which needs to load data.

Tech Stack:
- Orchestrator - Airflow
- Data Lake - S3
- DWH - Redshift
- Computing - native Airflow workers OR EMR (large jobs) or Containers to separate storage

Libraries:
- airflow;
- requests or a specific API SDK
- fireducks/ polars/ duckdb/ pandas;
- boto3;

## Data Flow
- Extract from API;
- Load to dataframe;
- Load to S3;
- COPY from S3 to Redshift;

# Requirements
## Functional

Data will be taken from API for specific time range, which possibly can be taken from airflow context (e.g. execution_time), delta of schedules, custom dates.
Data will be loaded to partitioned folders (e.g. /api_name/partition_date/), which can be overwritten in case of repeatable action.
Regarding dwh, we would implement deleting partition before inserting to make sure it don't exist.


### Generalized
We would want to create connectors to different APIs based on the parent connector.
Dags will be generated by template, to be able to make similar dags with different APIs.

### Reusability
DAGs should support parameterized manual runs/ scheduled runs.

### Testability
Logic will be wrapped with unit tests.
Additional data test checks could be implemented as a part of the pipeline (e.g. GreatExpectations)

### Readability
Going to use well-named, modular Airflow parts. The logic will be clearly broken down.

### Failure management
Airflow tasks provide us possibility to make it easily rerunable from failed step. Except that, we would implement logic for notifying ops in messengers/ executing some additional code.

### Logging
Using Python's logging module, surfaced through Airflow logs, along with all needed details.
